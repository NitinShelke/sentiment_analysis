{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project import logger\n",
    "from project.utils.common import clean_data,read_yaml_file,read_data_from_s3\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,recall_score,precision_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "import boto3\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=read_yaml_file(\"config/config.yaml\")\n",
    "s3=boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.mlflow.experiment_name=config.mlflow.experiment_name.split(\"_\")[0]+\"_\"+str(int(config.mlflow.experiment_name.split(\"_\")[1])+1)\n",
    "config.to_yaml(\"config/config.yaml\")\n",
    "config=read_yaml_file(\"config/config.yaml\")\n",
    "\n",
    "bucket=config.s3.bucket\n",
    "data_object=config.s3.save_object\n",
    "tracking_uri=config.mlflow.tracking_uri\n",
    "experiment_name=config.mlflow.experiment_name\n",
    "model_name=config.mlflow.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(tracking_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/03 12:12:50 INFO mlflow.tracking.fluent: Experiment with name 'Experiment_9' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://sentimet-analysis/artifacts/9', creation_time=1719988972095, experiment_id='9', last_update_time=1719988972095, lifecycle_stage='active', name='Experiment_9', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=read_data_from_s3(bucket,data_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(data['Sentiment'])\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(tfidf_df,data['Sentiment'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=MultinomialNB()\n",
    "svc=SVC()\n",
    "models={'naive_baise':nb,'Support_Vecor_classifies':svc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(model, model_name):\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_param(\"model_name\",model_name)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "        # Log the model\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        mlflow.log_artifact(\"\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    logger.info(\">>>>>>>>>>Experiment started>>>>>>>>>>\")\n",
    "    for model in models:\n",
    "        try:\n",
    "            train_and_log_model(models[model], model)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Something went wrong {}\".format(e))\n",
    "        \n",
    "    logger.info(\">>>>>>>>>>>Experiment completed\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_model():\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    try:\n",
    "        # Get the experiment\n",
    "        experiment = client.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            raise ValueError(f\"Experiment {experiment_name} not found.\")\n",
    "        \n",
    "        # Get all runs from the experiment\n",
    "        runs = client.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "        if not runs:\n",
    "            raise ValueError(f\"No runs found for experiment {experiment_name}.\")\n",
    "\n",
    "        # Find the best run based on accuracy\n",
    "        best_run = max(runs, key=lambda run: run.data.metrics.get('accuracy', 0))\n",
    "        best_run_id = best_run.info.run_id\n",
    "        latest_accuracy = best_run.data.metrics.get('accuracy', 0)\n",
    "\n",
    "        # Load the best model\n",
    "        best_model = mlflow.sklearn.load_model(f\"runs:/{best_run_id}/model\")\n",
    "\n",
    "        try:\n",
    "            # Check if there are any previously registered versions of the model\n",
    "            latest_versions = client.get_latest_versions(model_name, stages=[\"None\"])\n",
    "        except mlflow.exceptions.RestException as e:\n",
    "            if e.error_code == 'RESOURCE_DOES_NOT_EXIST':\n",
    "                latest_versions = []\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "        if not latest_versions:\n",
    "            # No previous versions, register the new model\n",
    "            model_uri = f\"runs:/{best_run_id}/model\"\n",
    "            mlflow.register_model(model_uri, model_name)\n",
    "            logger.info(\"Model has been registered.\")\n",
    "            \n",
    "        else:\n",
    "            # Check the accuracy of the latest registered model version\n",
    "            latest_version = latest_versions[-1]\n",
    "            run_id = latest_version.run_id\n",
    "            run = client.get_run(run_id)\n",
    "            last_accuracy = run.data.metrics.get(\"accuracy\", 0)\n",
    "\n",
    "            # Compare accuracies and register the new model if it's better\n",
    "            if latest_accuracy > last_accuracy:\n",
    "                model_uri = f\"runs:/{best_run_id}/model\"\n",
    "                mlflow.register_model(model_uri, model_name)\n",
    "                logger.info(\"Latest version of the model has been registered.\")\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                logger.info(\"Last version accuracy is greater than or equal to the latest version.\")\n",
    "\n",
    "        # Save the updated configuration\n",
    "        \n",
    "    except mlflow.exceptions.RestException as e:\n",
    "        logger.error(f\"Error during model registration: {e}\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 9/9 [00:01<00:00,  5.08it/s]\n"
     ]
    }
   ],
   "source": [
    "register_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
